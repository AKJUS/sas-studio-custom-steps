{"creationTimeStamp":"2023-02-21T17:09:16.189Z","modifiedTimeStamp":"2023-04-24T09:21:10.437Z","createdBy":"alfredo","modifiedBy":"alfredo","name":"Export - ADLS File Writer.step","displayName":"Export - ADLS File Writer.step","localDisplayName":"Export - ADLS File Writer.step","properties":{},"links":[{"method":"GET","rel":"self","href":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","uri":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","type":"application/vnd.sas.data.flow.step"},{"method":"GET","rel":"alternate","href":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","uri":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","type":"application/vnd.sas.data.flow.step.summary"},{"method":"GET","rel":"up","href":"/dataFlows/steps","uri":"/dataFlows/steps","type":"application/vnd.sas.collection","itemType":"application/vnd.sas.data.flow.step.summary"},{"method":"PUT","rel":"update","href":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","uri":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","type":"application/vnd.sas.data.flow.step","responseType":"application/vnd.sas.data.flow.step"},{"method":"DELETE","rel":"delete","href":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","uri":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238"},{"method":"GET","rel":"transferExport","href":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","uri":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","responseType":"application/vnd.sas.transfer.object"},{"method":"PUT","rel":"transferImportUpdate","href":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","uri":"/dataFlows/steps/bd4a2025-1c43-4e4f-8638-0b802c7db238","type":"application/vnd.sas.transfer.object","responseType":"application/vnd.sas.summary"}],"metadataVersion":0.0,"version":2,"type":"code","flowMetadata":{"inputPorts":[{"name":"intblid_source","displayName":"intblid_source","localDisplayName":"intblid_source","minEntries":1,"maxEntries":1,"type":"table"}],"outputPorts":[]},"ui":"{\n\t\"showPageContentOnly\": true,\n\t\"pages\": [\n\t\t{\n\t\t\t\"id\": \"pageConnection\",\n\t\t\t\"type\": \"page\",\n\t\t\t\"label\": \"Connection Settings\",\n\t\t\t\"children\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"intblid_source\",\n\t\t\t\t\t\"type\": \"inputtable\",\n\t\t\t\t\t\"label\": \"Input\",\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"Choose an input table\",\n\t\t\t\t\t\"readonly\": false\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"sectCredentials\",\n\t\t\t\t\t\"type\": \"section\",\n\t\t\t\t\t\"label\": \"ADLS credentials\",\n\t\t\t\t\t\"open\": true,\n\t\t\t\t\t\"visible\": \"\",\n\t\t\t\t\t\"children\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextTenantId\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the tentant id\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextClientid\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the client id\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextClientSec\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the client secret\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"sectStorageOpts\",\n\t\t\t\t\t\"type\": \"section\",\n\t\t\t\t\t\"label\": \"Storage options\",\n\t\t\t\t\t\"open\": true,\n\t\t\t\t\t\"visible\": \"\",\n\t\t\t\t\t\"children\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextStorageAccount\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the storage account\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextContainer\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the container\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextFilePath\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the path\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"id\": \"pgOptions\",\n\t\t\t\"type\": \"page\",\n\t\t\t\"label\": \"Options\",\n\t\t\t\"children\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownFormat\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Output format:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"parquet\",\n\t\t\t\t\t\t\t\"label\": \"Parquet\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownExistingDataBeh\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Existing data behavior:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"error\",\n\t\t\t\t\t\t\t\"label\": \"Error\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"overwrite_or_ignore\",\n\t\t\t\t\t\t\t\"label\": \"Overrite or Ignore\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"delete_matching\",\n\t\t\t\t\t\t\t\"label\": \"Delete Matching\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"chkCompression\",\n\t\t\t\t\t\"type\": \"checkbox\",\n\t\t\t\t\t\"label\": \"Enable compression\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownCompressCodec\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Compression type:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"snappy\",\n\t\t\t\t\t\t\t\"label\": \"Snappy\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": \"$chkCompression\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"chkPartitioned\",\n\t\t\t\t\t\"type\": \"checkbox\",\n\t\t\t\t\t\"label\": \"Partition Table\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"colselPartitionCols\",\n\t\t\t\t\t\"type\": \"columnselector\",\n\t\t\t\t\t\"label\": \"Partition columns:\",\n\t\t\t\t\t\"order\": true,\n\t\t\t\t\t\"columntype\": \"a\",\n\t\t\t\t\t\"max\": 4,\n\t\t\t\t\t\"min\": 1,\n\t\t\t\t\t\"visible\": \"$chkPartitioned\",\n\t\t\t\t\t\"table\": \"intblid_source\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"id\": \"pgAbout\",\n\t\t\t\"type\": \"page\",\n\t\t\t\"label\": \"About\",\n\t\t\t\"children\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"outtxtAbout\",\n\t\t\t\t\t\"type\": \"text\",\n\t\t\t\t\t\"text\": \"The Export - ADLS File Writer provides an easy way to write SAS and CAS Datasets to Azure Data Lake Storage (ADLS) in Parquet format.\\n\\nIt supports writing compressed Parquet files using the snappy compression to reduce storage requirements.\\nIt also supports writing partitioned parquet datasets based in a particular column or set of columns. This allows for more efficient querying and processing of large datasets, as only the relevant partitions need to be accessed. \\n\\nTo control how to handle data that already exists in the destination the field Existing data behavior is provided with the following configuration alternatives:\\n*  overwrite_or_ignore: will ignore any existing data and will overwrite files with the same name as an output file. Other existing files will be ignored. This behavior, in combination with a unique basename_template for each write, will allow for an append workflow.\\n*  error: will raise an error if any data exists in the destination.\\n*  delete_matching: is useful when you are writing a partitioned dataset. The first time each partition directory is encountered the entire directory will be deleted. This allows you to overwrite old partitions completely. \\n\\nThis custom step helps to work around some of the restrictions that currently exist for working with Parquet files in SAS Viya. Please check the following documentation that lists those restrictions for the latest SAS Viya release:\\n\\n*  Restrictions for Parquet File Features for the libname engine (SAS Compute Server)- https://go.documentation.sas.com/doc/en/pgmsascdc/default/enghdff/p1pr85ltrpplbtn1h9sog99p4mr5.htm\\n*  Azure Data Lake Storage Data Source (SAS Cloud Analytic Services)-https://go.documentation.sas.com/doc/en/pgmsascdc/default/casref/n1ogaeli0qbctqn1e3fx8gz70lkq.htm\\n*  Path-Based Data Source Types and Options â€“ which has a footnote for Parquet (SAS Cloud Analytic Services)-https://go.documentation.sas.com/doc/en/pgmsascdc/default/casref/n0kizq68ojk7vzn1fh3c9eg3jl33.htm#n0cxk3edba75w8n1arx3n0dxtdrt\\n\\nThis custom step depends on having a python environment configured with some additional libraries installed. It has been tested on SAS Viya version Stable 2023.03 with python version 3.8.13 and the following libraries versions:\\n\\n- pandas == 1.5.2         \\n- saspy == 4.3.3          \\n- azure-identity == 1.12.0\\n- pyarrow == 10.0.1       \\n- adlfs == 2023.1.0       \\n\\nVersion 1.0 (20APR2023)\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t],\n\t\"syntaxversion\": \"1.3.0\",\n\t\"values\": {\n\t\t\"intblid_source\": {\n\t\t\t\"library\": \"\",\n\t\t\t\"table\": \"\"\n\t\t},\n\t\t\"intextTenantId\": \"\",\n\t\t\"intextClientid\": \"\",\n\t\t\"intextClientSec\": \"\",\n\t\t\"intextStorageAccount\": \"\",\n\t\t\"intextContainer\": \"\",\n\t\t\"intextFilePath\": \"\",\n\t\t\"dpdownFormat\": {\n\t\t\t\"value\": \"parquet\",\n\t\t\t\"label\": \"Parquet\"\n\t\t},\n\t\t\"dpdownExistingDataBeh\": {\n\t\t\t\"value\": \"error\",\n\t\t\t\"label\": \"Error\"\n\t\t},\n\t\t\"chkCompression\": false,\n\t\t\"dpdownCompressCodec\": {\n\t\t\t\"value\": \"snappy\",\n\t\t\t\"label\": \"Snappy\"\n\t\t},\n\t\t\"chkPartitioned\": false,\n\t\t\"colselPartitionCols\": []\n\t}\n}","templates":{"SAS":"/***********************************************************************\n Custom Step for reading parquet files from ADLS, supports reading from \n partitioned directory structure and also can apply filtering criteria\n up to 3 conditions.\n\n***********************************************************************/\n\nproc python terminate;\nsubmit;\n\nimport logging\nfrom abc import ABCMeta\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom adlfs import AzureBlobFileSystem\nfrom saspy import SASsession\n\n\n# -------------  INI Utilities --------------------------------------------------------------------------\nclass ADLSFilerWriter(metaclass=ABCMeta):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self,\n                 SAS: SASsession,\n                 tenant_id: str,\n                 client_id: str,\n                 client_secret: str,\n                 account_name: str,\n                 container: str,\n                 existing_data_behavior: str,\n                 compression_codec: str,\n                 partition_table: bool,\n                 sas_part_selector_id: str):\n        \"\"\"\n\n        :param SAS:\n        :param tenant_id:\n        :param client_id:\n        :param client_secret:\n        :param account_name:\n        :param container:\n        :param existing_data_behavior:\n        :param compression_codec:\n        :param partition_table:\n        :param sas_part_selector_id:\n        \"\"\"\n\n        self._logger = logging.getLogger('sas.customsteps.adlsFileWriter')\n\n        if SAS is None:\n            self._logger.error(\"The SAS object is mandatory\")\n            raise ValueError(\"The SAS object is mandatory\")\n        else:\n            self._SAS = SAS\n\n        self._container = container\n\n        self._filesystem = AzureBlobFileSystem(\n            account_name=account_name,\n            tenant_id=tenant_id,\n            client_id=client_id,\n            client_secret=client_secret\n        )\n\n        if compression_codec not in [None, 'snappy']:\n            self._logger.error(\"The param 'compression_codec' should be one of ['None', 'snappy'].\")\n            raise ValueError(\"compression_codec should be one of ['None', 'snappy'].\")\n        else:\n            self._compression_codec = compression_codec\n\n        if existing_data_behavior not in ['error', 'overwrite_or_ignore', 'delete_matching']:\n            self._logger.error(\"The param 'existing_data_behavior' should be one of ['error', 'overwrite_or_ignore', \"\n                               \"'delete_matching'].\")\n            raise ValueError(\"existing_data_behavior should be one of ['error', 'overwrite_or_ignore', \"\n                             \"'delete_matching'].\")\n        else:\n            self._existing_data_behavior = existing_data_behavior\n\n        if partition_table:\n            sas_part_selector = self._SAS.symget(f\"{sas_part_selector_id}\")\n            self._logger.debug(\n                f\" SAS macro '{sas_part_selector_id}' Type: {type(sas_part_selector_id)}, Value: {sas_part_selector_id}\")\n\n            if sas_part_selector is None or sas_part_selector == \"\":\n                raise ValueError(\n                    f\"When 'partition' value is True the {sas_part_selector_id} macro {sas_part_selector_id} was not found on the SASsession\")\n            else:\n                self._sas_part_selector_id = sas_part_selector_id\n        else:\n            self._sas_part_selector_id = None\n\n    def __generate_partitions_list(self, sas_part_selector_id):\n        \"\"\"\n\n        :param sas_part_selector_id:\n        :return:\n        \"\"\"\n\n        self._logger.debug(\n            f\" SAS macro 'sas_part_selector_id' Type: {type(sas_part_selector_id)}, Value: {sas_part_selector_id}\")\n\n        if sas_part_selector_id is None:\n            return None\n\n        sas_part_selector_id_count = self._SAS.symget(f\"{sas_part_selector_id}_count\")\n        self._logger.info(\n            f\" SAS macro '{sas_part_selector_id}_count' Type: {type(sas_part_selector_id_count)}, Value: {sas_part_selector_id_count}\")\n\n        partitions_no = int(sas_part_selector_id_count)\n        self._logger.info(f\" Python var 'partitions_no' Type: {type(partitions_no)}, Value: {partitions_no}\")\n        if partitions_no is None or partitions_no == \"\":\n            return None\n        else:\n            partitions = []\n            for i in range(1, partitions_no + 1):\n                column_name = self._SAS.symget(f\"{sas_part_selector_id}_{i}_name\")\n                self._logger.debug(\n                    f\" SAS macro '{sas_part_selector_id}_{i}_name' Type: {type(column_name)}, Value: {column_name}\")\n\n                partitions.append(column_name)\n\n            self._logger.info(f\" result 'partitions' Type: {type(partitions)}, Value: {partitions}\")\n            return partitions\n\n    def write(self, table, path):\n        # https://arrow.apache.org/docs/10.0/python/generated/pyarrow.dataset.write_dataset.html\n\n        if isinstance(table, pd.DataFrame):\n            pyarr_table = pa.Table.from_pandas(table)\n        elif isinstance(table, pa.Table):\n            pyarr_table = table\n        else:\n            raise f\"The parameter 'table' type must be one of [pandas.DataFrame, pyarrow.Table]\"\n\n        if path is None or path == \"\":\n            raise \"The path argument can not be empty\"\n\n        root_path = path[:-1] if path.endswith(\"/\") else path\n\n        partitions = self.__generate_partitions_list(\n            self._sas_part_selector_id if self._sas_part_selector_id is not None else None\n        )\n\n        self._logger.debug(f\"\"\" Write to Dataset: \n                                root_path: '{root_path}'\n                                filesystem: {type(self._filesystem)}\n                                file_format:{self._compression_codec}\n                                existing_data_behavior:{self._existing_data_behavior}\n                                partition_cols: {partitions}\n                            \"\"\")\n\n        if self._compression_codec == \"snappy\":\n            pq.write_to_dataset(\n                pyarr_table,\n                root_path=f\"{self._container}/{root_path}\",\n                filesystem=self._filesystem,\n                existing_data_behavior=self._existing_data_behavior,\n                partition_cols=partitions\n            )\n        else:\n            pq.write_to_dataset(\n                pyarr_table,\n                root_path=f\"{self._container}/{root_path}\",\n                filesystem=self._filesystem,\n                compression=self._compression_codec,\n                existing_data_behavior=self._existing_data_behavior,\n                partition_cols=partitions\n            )\n\n# -------------  END Utilities --------------------------------------------------------------------------\n\n\n\n# -------------  INI CustomStep Logic --------------------------------------------------------------------\nlogging.basicConfig(level=logging.ERROR)\n\n# create logger\nlogger = logging.getLogger('sas.customsteps.adlsFileWriter')\nlogger.setLevel(logging.INFO)\n\ntenant_id = SAS.symget('intextTenantId')\nclient_id = SAS.symget('intextClientid')\nclient_secret = SAS.symget('intextClientSec')\nstorage_account_name = SAS.symget('intextStorageAccount')\ncontainer_name = SAS.symget('intextContainer')\nintextFilePath = SAS.symget('intextFilePath')\ndpdownFormat = SAS.symget('dpdownFormat')\nchkCompression = SAS.symget('chkCompression')\nchkPartitioned = SAS.symget('chkPartitioned')\n\nlogger.info(f\" SAS macro 'intextTenantId' Type: {type(tenant_id)}, Value: {tenant_id}\")\nlogger.info(f\" SAS macro 'intextClientid' Type: {type(client_id)}, Value: {client_id}\")\nlogger.info(f\" SAS macro 'intextClientSec' Type: {type(client_secret)}, Value: {client_secret}\")\nlogger.info(f\" SAS macro 'intextStorageAccount' Type: {type(storage_account_name)}, Value: {storage_account_name}\")\nlogger.info(f\" SAS macro 'intextContainer' Type: {type(container_name)}, Value: {container_name}\")\nlogger.info(f\" SAS macro 'intextFilePath' Type: {type(intextFilePath)}, Value: {intextFilePath}\")\nlogger.info(f\" SAS macro 'dpdownFormat' Type: {type(dpdownFormat)}, Value: {dpdownFormat}\")\nlogger.info(f\" SAS macro 'chkCompression' Type: {type(chkCompression)}, Value: {chkCompression}\")\nlogger.info(f\" SAS macro 'chkPartitioned' Type: {type(chkPartitioned)}, Value: {chkPartitioned}\")\n\nuseCompression = (int(chkCompression) == 1)\nisParted = (int(chkPartitioned) == 1)\n\nlogger.info(f\" Python var 'useCompression' Type: {type(useCompression)}, Value: {useCompression}\")\nlogger.info(f\" Python var 'isParted' Type: {type(isParted)}, Value: {isParted}\")\n\n\nuseCompression = (int(chkCompression) == 1)\nisParted = (int(chkPartitioned) == 1)\n\nlogger.info(f\" Python var 'useCompression' Type: {type(useCompression)}, Value: {useCompression}\")\nlogger.info(f\" Python var 'isParted' Type: {type(isParted)}, Value: {isParted}\")\n\ndpdownExistingDataBeh = SAS.symget('dpdownExistingDataBeh')\ndpdownCompressCodec = SAS.symget('dpdownCompressCodec')\n\nintblid_source = SAS.symget('intblid_source')\ninputdf = SAS.sd2df(intblid_source)\n\nADLSFilerWriter(\n    SAS=SAS,\n    tenant_id=tenant_id,\n    client_id=client_id,\n    client_secret=client_secret,\n    account_name=storage_account_name,\n    container=container_name,\n    existing_data_behavior=dpdownExistingDataBeh,\n    compression_codec=dpdownCompressCodec if useCompression else None,\n    partition_table=isParted,\n    sas_part_selector_id=\"colselPartitionCols\"\n).write(table=inputdf, path=intextFilePath)\n# -------------  END CustomStep Logic --------------------------------------------------------------------\n\nendsubmit;\nquit;\n\n\n"}}